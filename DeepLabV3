import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
from PIL import Image
import os
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Resim ve Maske Yollarını Almak
image_dir = r'/content/drive/MyDrive/ETIS-LaribPolypDB/images'  # Resimlerin bulunduğu klasör
mask_dir = r'/content/drive/MyDrive/ETIS-LaribPolypDB/masks'    # Maskelerin bulunduğu klasör

# Dosyaları alıyoruz
image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.endswith('.png') or filename.endswith('.jpg')]
mask_paths = [os.path.join(mask_dir, filename) for filename in os.listdir(mask_dir) if filename.endswith('.png') or filename.endswith('.jpg')]

# Dosyaların sıralandığından emin olun
image_paths.sort()
mask_paths.sort()

# Kontrol için ilk 5 yolu yazdıralım
print("Image paths:", image_paths[:5])
print("Mask paths:", mask_paths[:5])

# Dataset Sınıfı
class PolypDataset(Dataset):
    def __init__(self, image_paths, mask_paths, transform=None):
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert("RGB")
        mask = Image.open(self.mask_paths[idx]).convert("L")  # Maskeleri gri tonlamalı olarak yükle

        if self.transform:
            image = self.transform(image)
            mask = self.transform(mask)

        return image, mask


# Veriyi yüklemek için transformlar
transform = transforms.Compose([
    transforms.Resize((256, 256)),  # Resim ve maskeleri 256x256 boyutuna yeniden boyutlandır
    transforms.ToTensor(),
])

# Dataset oluşturma
dataset = PolypDataset(image_paths, mask_paths, transform=transform)

# Eğitim ve doğrulama verilerini ayırma
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# DataLoader'lar (batch_size=4 ve drop_last=True)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, drop_last=True)

# Modeli Tanımlama (DeepLabV3+)
model = models.segmentation.deeplabv3_resnet101(pretrained=True)
model.classifier[4] = torch.nn.Conv2d(256, 2, kernel_size=(1, 1))  # 2 sınıf (polip ve arka plan)
model = model.to('cuda')

# Optimizasyon ve Kayıp Fonksiyonu
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = torch.nn.CrossEntropyLoss()

# Metrikleri saklamak için listeler
train_losses = []
val_losses = []
accuracies = []
precisions = []
recalls = []
f1_scores = []

# Early Stopping için değişkenler
best_val_loss = float('inf')  # En iyi validation loss'u saklamak için
patience = 5  # Kaç epoch boyunca iyileşme olmazsa durdurulacağını belirler
early_stopping_counter = 0  # İyileşme olmayan epoch sayısını sayar

# Eğitim Döngüsü
num_epochs = 20
for epoch in range(num_epochs):
    model.train()  # Modeli eğitim moduna al
    running_loss = 0.0
    all_preds = []
    all_labels = []

    for images, masks in train_loader:
        images = images.to('cuda')
        masks = masks.to('cuda')

        # Maskeyi uygun formata getirme: Kanal boyutunu sıkıştırma
        masks = masks.squeeze(1)  # Maskenin son kanalını kaldırıyoruz (resimler 3 kanaldıdır, ancak maskeler genellikle tek kanaldır)
        masks = masks.long()  # Maskeyi integer (long) tipine dönüştürme

        optimizer.zero_grad()  # Gradients sıfırlama
        output = model(images)['out']  # Modelin çıkışı

        # Kayıp hesapla
        loss = criterion(output, masks)  # CrossEntropyLoss için [batch_size, num_classes, height, width] ile [batch_size, height, width]
        loss.backward()  # Backpropagation
        optimizer.step()  # Ağırlıkları güncelle

        running_loss += loss.item()

        # Tahminleri ve etiketleri topla
        preds = torch.argmax(output, dim=1).cpu().numpy().flatten()
        labels = masks.cpu().numpy().flatten()
        all_preds.extend(preds)
        all_labels.extend(labels)

    # Epoch sonu metrikleri hesapla
    epoch_loss = running_loss / len(train_loader)
    train_losses.append(epoch_loss)

    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='macro')
    recall = recall_score(all_labels, all_preds, average='macro')
    f1 = f1_score(all_labels, all_preds, average='macro')

    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1)

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}")

    # Validation Loss Hesaplama
    model.eval()  # Modeli değerlendirme moduna al
    val_loss = 0.0
    with torch.no_grad():
        for images, masks in val_loader:
            images = images.to('cuda')
            masks = masks.to('cuda')
            masks = masks.squeeze(1).long()
            output = model(images)['out']
            val_loss += criterion(output, masks).item()

    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    # Early Stopping Kontrolü
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stopping_counter = 0  # İyileşme oldu, sayacı sıfırla
        torch.save(model.state_dict(), 'best_model.pth')  # En iyi modeli kaydet
    else:
        early_stopping_counter += 1  # İyileşme olmadı, sayacı artır

    # Eğer belirli bir sayıda epoch boyunca iyileşme olmazsa eğitimi durdur
    if early_stopping_counter >= patience:
        print(f"Early stopping triggered at epoch {epoch+1}!")
        break

# Modeli kaydetme (isteğe bağlı)
torch.save(model.state_dict(), 'deeplabv3_polyp_model.pth')

# Grafikleri çizme
plt.figure(figsize=(12, 8))

# Loss grafiği
plt.subplot(2, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

# Accuracy grafiği
plt.subplot(2, 2, 2)
plt.plot(accuracies, label='Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()

# Precision grafiği
plt.subplot(2, 2, 3)
plt.plot(precisions, label='Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.title('Precision')
plt.legend()

# F1 Score grafiği
plt.subplot(2, 2, 4)
plt.plot(f1_scores, label='F1 Score')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.title('F1 Score')
plt.legend()

plt.tight_layout()
plt.show()
